{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install wordcloud lazypredict"
      ],
      "metadata": {
        "id": "4FLHxy-_IxiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing all the necessary packages\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "v00HsN9NSGUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "id": "C3mnM1O8RjQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "\n",
        "from sklearn.preprocessing import scale, StandardScaler\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, r2_score, roc_auc_score, roc_curve, classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.model_selection import KFold\n",
        "import warnings\n",
        "warnings.simplefilter(action = \"ignore\")\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import svm\n",
        "from lazypredict.Supervised import LazyClassifier, LazyRegressor\n",
        "import pickle\n",
        "import time"
      ],
      "metadata": {
        "id": "qfImLDx-Iqwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " import nltk\n",
        "\n",
        " nltk.download()"
      ],
      "metadata": {
        "id": "COlamk-KsJaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "# Reset the output dimensions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import decomposition\n",
        "from sklearn.svm import LinearSVC, NuSVC, SVC\n",
        "from sklearn.metrics import f1_score, accuracy_score, hamming_loss\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction import text\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, HashingVectorizer\n",
        "\n",
        "\n",
        "from scipy import linalg\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import pickle\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk import stem\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer, SnowballStemmer\n",
        "\n",
        "from gensim import matutils, models\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import scipy.sparse\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import seaborn as sns\n",
        "\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [24, 12]\n",
        "plt.style.use('seaborn-darkgrid')"
      ],
      "metadata": {
        "id": "xOS4i8Z7ruMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Mount Google Drive (if using Google Colab)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "FiscpGShTaT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train  = pd.read_csv('/content/drive/MyDrive/hate speech/train.csv')\n",
        "print(\"Training Set 1 :\"% train.columns, train.shape)\n",
        "test = pd.read_csv('/content/drive/MyDrive/hate speech/train.csv')\n",
        "print(\"Test Set 1 :\"% test.columns, test.shape)\n",
        "\n",
        "print('Train Set 1 -----')\n",
        "print(train.isnull().sum())\n",
        "print('Test Set 1 -----')\n",
        "print(test.isnull().sum())\n",
        "train.head()"
      ],
      "metadata": {
        "id": "gfkWszY7IqzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/hate speech/our hate speech combined.csv')\n"
      ],
      "metadata": {
        "id": "oGK0YaeJIq2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Before renaming the columns\n",
        "print(\"\\nBefore modifying column names:\\n\", df.columns)\n",
        "\n",
        "df.rename(columns = {'Hate =1 0t Hate=0':'label','Type':'catagory'}, inplace = True)\n",
        "\n",
        "# After renaming the columns\n",
        "print(\"\\nAfter modifying first column:\\n\", df.columns)"
      ],
      "metadata": {
        "id": "bHtjXcZdVQDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting the Dataset into Train and Test dataset\n",
        "\n",
        "train, test = train_test_split(df, test_size=0.3,random_state=10, shuffle=True)\n",
        "\n",
        "train = train[['Serial', 'Comment', 'label', 'catagory']]\n",
        "test = test[['Serial', 'Comment']]\n",
        "\n",
        "print(\"Training Set 2 :\"% train.columns, train.shape)\n",
        "print(\"Test Set 2 :\"% test.columns, test.shape)\n",
        "print('Train Set 2 -----')\n",
        "print(train.isnull().sum())\n",
        "print('Test Set 2 -----')\n",
        "print(test.isnull().sum())\n",
        "train.head()"
      ],
      "metadata": {
        "id": "YcZJuMOSIq-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data analysis"
      ],
      "metadata": {
        "id": "UpW2CTEeWrhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def  clean_text(df, text_field):\n",
        "    df[text_field] = df[text_field].str.lower()\n",
        "    df[text_field] = df[text_field].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))\n",
        "    return df\n",
        "test_clean = clean_text(test, \"Comment\")\n",
        "train_clean = clean_text(train, \"Comment\")\n",
        "\n",
        "train_majority = train_clean[train_clean.label==0]\n",
        "train_minority = train_clean[train_clean.label==1]\n",
        "train_minority_upsampled = resample(train_minority,\n",
        "                                 replace=True,\n",
        "                                 n_samples=len(train_majority),\n",
        "                                 random_state=123)\n",
        "train_upsampled = pd.concat([train_minority_upsampled, train_majority])\n",
        "train_upsampled['label'].value_counts()"
      ],
      "metadata": {
        "id": "oHWQOfW8bcxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_upsampled"
      ],
      "metadata": {
        "id": "8BcvSVVNbcu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualizing the Data"
      ],
      "metadata": {
        "id": "BfqZuJ4hYOEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Group the data by category and count the occurrences\n",
        "category_counts = df['catagory'].value_counts()"
      ],
      "metadata": {
        "id": "NyXlvXQvDbje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize = (50,50))\n",
        "sns.countplot(x=df['catagory'])\n",
        "\n",
        "plt.xlabel('Catagory', fontweight = 'bold' )\n",
        "plt.ylabel('Count', fontweight = 'bold' )\n",
        "plt.title('Count of Category Distribution (Unique Values)', fontweight = 'bold' )\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Od3_IBugNJmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the occurrences of unique categories\n",
        "category_counts = df['catagory'].value_counts()\n",
        "\n",
        "# Create a bar plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "category_counts.plot(kind='bar')\n",
        "plt.title('Category Distribution (Unique Values)')\n",
        "plt.xlabel('catagory')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Fm1VKnbSDzhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.concat([train,test])\n",
        "data.columns = ['Serial', 'Comment', 'label', 'catagory']\n",
        "data['Label'] = np.where(data['label'] == 'neg', 1, 0)\n",
        "\n",
        "\n",
        "df = df.append(data , ignore_index = True)\n",
        "\n",
        "sns.countplot(x=\"label\",data=df)\n",
        "print(len(df))\n"
      ],
      "metadata": {
        "id": "kvXqNoTIfOF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(inplace=True)\n",
        "print(df.isnull().sum().sum())"
      ],
      "metadata": {
        "id": "KwwwtYpWgWr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.duplicated().sum().sum())  # Number Of Dupliactes Before Delete\n",
        "df = df.drop_duplicates(keep='first')\n",
        "print(df.duplicated().sum().sum()) # Number Of Dupliactes After Delete"
      ],
      "metadata": {
        "id": "Bc2cy1E9gl0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['catagory'] = df['catagory'].astype(str)\n",
        "df['Label'] = df['Label'].astype(int)\n",
        "# print(df.head())\n",
        "\n",
        "sns.countplot(x=\"Label\",data=df)\n",
        "print(len(df))"
      ],
      "metadata": {
        "id": "kvX62PfgglxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word_count\n",
        "train['word_count'] = train['Comment'].apply(lambda x: len(str(x).split()))\n",
        "test['word_count'] = test['Comment'].apply(lambda x: len(str(x).split()))\n",
        "\n",
        "# unique_word_count\n",
        "train['unique_word_count'] = train['Comment'].apply(lambda x: len(set(str(x).split())))\n",
        "test['unique_word_count'] = test['Comment'].apply(lambda x: len(set(str(x).split())))\n",
        "\n",
        "# stop_word_count\n",
        "train['stop_word_count'] = train['Comment'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n",
        "test['stop_word_count'] = test['Comment'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n",
        "\n",
        "# url_count\n",
        "train['url_count'] = train['Comment'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n",
        "test['url_count'] = test['Comment'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n",
        "\n",
        "# mean_word_length\n",
        "train['mean_word_length'] = train['Comment'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
        "test['mean_word_length'] = test['Comment'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
        "\n",
        "# char_count\n",
        "train['char_count'] = train['Comment'].apply(lambda x: len(str(x)))\n",
        "test['char_count'] = test['Comment'].apply(lambda x: len(str(x)))\n",
        "\n",
        "# punctuation_count\n",
        "train['punctuation_count'] = train['Comment'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
        "test['punctuation_count'] = test['Comment'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
        "\n",
        "# hashtag_count\n",
        "train['hashtag_count'] = train['Comment'].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
        "test['hashtag_count'] = test['Comment'].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
        "\n",
        "# mention_count\n",
        "train['mention_count'] = train['Comment'].apply(lambda x: len([c for c in str(x) if c == '@']))\n",
        "test['mention_count'] = test['Comment'].apply(lambda x: len([c for c in str(x) if c == '@']))"
      ],
      "metadata": {
        "id": "u8sXwSN8q9mW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "METAFEATURES = ['word_count', 'unique_word_count', 'stop_word_count', 'url_count', 'mean_word_length',\n",
        "        'char_count', 'punctuation_count', 'hashtag_count', 'mention_count']\n",
        "\n",
        "TWEETS = train['label'] == 1\n",
        "\n",
        "fig, axes = plt.subplots(ncols = 2, nrows = len(METAFEATURES), figsize = (20, 50), dpi = 100)\n",
        "\n",
        "for i, feature in enumerate(METAFEATURES):\n",
        "    sns.distplot(train.loc[~TWEETS][feature], label = 'Positive', ax = axes[i][0], color = 'green')\n",
        "    sns.distplot(train.loc[TWEETS][feature], label = 'Negative', ax = axes[i][0], color = 'red')\n",
        "\n",
        "    sns.distplot(train[feature], label = 'Training', ax = axes[i][1])\n",
        "    sns.distplot(test[feature], label = 'Test', ax = axes[i][1])\n",
        "\n",
        "    for j in range(2):\n",
        "        axes[i][j].set_xlabel('')\n",
        "        axes[i][j].tick_params(axis = 'x', labelsize = 12)\n",
        "        axes[i][j].tick_params(axis = 'y', labelsize = 12)\n",
        "        axes[i][j].legend()\n",
        "\n",
        "    axes[i][0].set_title(f'{feature} Target Distribution in Training Set', fontsize = 13)\n",
        "    axes[i][1].set_title(f'{feature} Training & Test Set Distribution', fontsize = 13)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V3_r52PKglux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(ncols = 2, figsize = (17, 4), dpi = 100)\n",
        "plt.tight_layout()\n",
        "\n",
        "train.groupby('label').count()['Serial'].plot(kind = 'pie', ax = axes[0], labels = ['Negative (92.9%)', 'Positive (7.1%)'])\n",
        "sns.countplot(x = train['label'], hue = train['label'], ax = axes[1])\n",
        "\n",
        "axes[0].set_ylabel('')\n",
        "axes[1].set_ylabel('')\n",
        "axes[1].set_xticklabels(['Negative (6000)', 'Positive (3242)'])\n",
        "axes[0].tick_params(axis = 'x', labelsize = 15)\n",
        "axes[0].tick_params(axis = 'y', labelsize = 15)\n",
        "axes[1].tick_params(axis = 'x', labelsize = 15)\n",
        "axes[1].tick_params(axis = 'y', labelsize = 15)\n",
        "\n",
        "axes[0].set_title('Label Distribution in Training Set', fontsize = 13)\n",
        "axes[1].set_title('Label Count in Training Set', fontsize = 13)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RY7oXATTglrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(1,2 , figsize=(16,8))\n",
        "text_pos = \" \".join(train_clean['Comment'][train.label == 0])\n",
        "text_neg = \" \".join(train_clean['Comment'][train.label == 1])\n",
        "train_cloud_pos = WordCloud(collocations = False, background_color = 'white').generate(text_pos)\n",
        "train_cloud_neg = WordCloud(collocations = False, background_color = 'black').generate(text_neg)\n",
        "axs[0].imshow(train_cloud_pos, interpolation='bilinear')\n",
        "axs[0].axis('off')\n",
        "axs[0].set_title('Non-Hate Comments')\n",
        "axs[1].imshow(train_cloud_neg, interpolation='bilinear')\n",
        "axs[1].axis('off')\n",
        "axs[1].set_title('Hate Comments')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZD6pqsMa0OlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(16,8))\n",
        "sns.set_style('darkgrid')\n",
        "sns.histplot(data = train['label'], color='black', legend=True)\n",
        "sns.histplot(data = train_upsampled['label'], color = 'orange', legend=True)\n",
        "plt.legend(['Initial_Data', 'Resampled_Data'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "266qIlaU0mIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('--------------After Upsampling the Minority Class most frequent word---------------')\n",
        "\n",
        "fig, axs = plt.subplots(1,2 , figsize=(16,8))\n",
        "text_pos = \" \".join(train_upsampled['Comment'][train.label == 0])\n",
        "text_neg = \" \".join(train_upsampled['Comment'][train.label == 1])\n",
        "train_cloud_pos = WordCloud(collocations = False, background_color = 'white').generate(text_pos)\n",
        "train_cloud_neg = WordCloud(collocations = False, background_color = 'black').generate(text_neg)\n",
        "axs[0].imshow(train_cloud_pos, interpolation='bilinear')\n",
        "axs[0].axis('off')\n",
        "axs[0].set_title('Non-Hate Comments')\n",
        "axs[1].imshow(train_cloud_neg, interpolation='bilinear')\n",
        "axs[1].axis('off')\n",
        "axs[1].set_title('Hate Comments')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Lc7NrOc70s4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair plot analysis\n",
        "sns.pairplot(data,hue='label',diag_kind='kde')"
      ],
      "metadata": {
        "id": "jUy6-SYk59MV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt_trasformed = train_upsampled[['label', 'Comment']]\n",
        "y = dt_trasformed.iloc[:, :-1].values"
      ],
      "metadata": {
        "id": "LcTwBabh1Bu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
        "y = np.array(ct.fit_transform(y))\n",
        "\n",
        "y"
      ],
      "metadata": {
        "id": "P_2Q70zy1HT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y.shape)"
      ],
      "metadata": {
        "id": "Ux7SRFLh1LxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_df = pd.DataFrame(y)\n",
        "y_hate = np.array(y_df[0])\n",
        "y"
      ],
      "metadata": {
        "id": "AZjUuaDw1QoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer(max_features = 2000)\n",
        "x = cv.fit_transform(train_upsampled['Comment']).toarray()\n",
        "x"
      ],
      "metadata": {
        "id": "0GN8i0KL1XHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "id": "WRzJXiSn1YvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preprocessing and Cleaning"
      ],
      "metadata": {
        "id": "cDqsUFD7uL8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(string):\n",
        "    word_list = [word.lower() for word in string.split()]\n",
        "    stopwords_list = list(stopwords.words(\"english\"))\n",
        "    for word in word_list:\n",
        "        if word in stopwords_list:\n",
        "            word_list.remove(word)\n",
        "    return ' '.join(word_list)"
      ],
      "metadata": {
        "id": "tFFFrxSYglk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['Comment'] = train['Comment'].apply(len)\n",
        "train['Comment'] = train['Comment'].map(lambda x: re.sub('\\\\n',' ',str(x)))\n",
        "train['Comment'] = train['Comment'].map(lambda x: re.sub(r'\\W',' ',str(x)))\n",
        "train['Comment'] = train['Comment'].map(lambda x: re.sub(r'https\\s+|www.\\s+',r'', str(x)))\n",
        "train['Comment'] = train['Comment'].map(lambda x: re.sub(r'http\\s+|www.\\s+',r'', str(x)))\n",
        "train['Comment'] = train['Comment'].map(lambda x: re.sub(r'\\s+[a-zA-Z]\\s+',' ',str(x)))\n",
        "train['Comment'] = train['Comment'].map(lambda x: re.sub(r'\\^[a-zA-Z]\\s+',' ',str(x)))\n",
        "train['Comment'] = train['Comment'].map(lambda x: re.sub(r'\\s+',' ',str(x)))\n",
        "train['Comment'] = train['Comment'].str.lower()\n",
        "\n",
        "train['Comment'] = train['Comment'].map(lambda x: re.sub(r\"\\’\", \"\\'\", str(x)))\n",
        "train['Comment'] = train['Comment'].map(lambda x: re.sub(r\"won\\'t\", \"will not\", str(x)))\n",
        "train['Comment'] = train['Comment'].map(lambda x: re.sub(r\"can\\'t\", \"can not\", str(x)))\n",
        "train['Comment'] = train['Comment'].map(lambda x: re.sub(r\"don\\'t\", \"do not\", str(x)))\n",
        "train['Comment'] = train['Comment'].map(lambda x: re.sub(r\"dont\", \"do not\", str(x)))\n",
        "train['Comment'] = train['Comment'].map(lambda x: re.sub(r\"n\\’t\", \" not\", str(x)))\n",
        "train['Comment'] = train['Comment'].map(lambda x: re.sub(r\"n\\'t\", \" not\", str(x)))\n",
        "train['Comment'] = train['Comment'].map(lambda x: re.sub(r\"\\'re\", \" are\", str(x)))\n",
        "train['Comment'] = train['Comment'].map(lambda x: re.sub(r\"\\'s\", \" is\", str(x)))\n",
        "train['Comment'] = train['Comment'].map(lambda x: re.sub(r\"\\’d\", \" would\", str(x)))\n",
        "train['Comment'] = train['Comment'].map(lambda x: re.sub(r\"\\d\", \" would\", str(x)))\n",
        "train['Comment'] = train['Comment'].map(lambda x: re.sub(r\"\\'ll\", \" will\", str(x)))\n",
        "train['Comment'] = train['Comment'].map(lambda x: re.sub(r\"\\'t\", \" not\", str(x)))\n",
        "train['Comment'] = train['Comment'].map(lambda x: re.sub(r\"\\'ve\", \" have\", str(x)))\n",
        "train['Comment'] = train['Comment'].map(lambda x: re.sub(r\"\\'m\", \" am\", str(x)))\n",
        "train['Comment'] = train['Comment'].map(lambda x: re.sub(r\"\\n\", \"\", str(x)))\n",
        "train['Comment'] = train['Comment'].map(lambda x: re.sub(r\"\\r\", \"\", str(x)))\n",
        "train['Comment'] = train['Comment'].map(lambda x: re.sub(r\"[0-9]\", \"digit\", str(x)))\n",
        "train['Comment'] = train['Comment'].map(lambda x: re.sub(r\"\\'\", \"\", str(x)))\n",
        "train['Comment'] = train['Comment'].map(lambda x: re.sub(r\"\\\"\", \"\", str(x)))\n",
        "train['Comment'] = train['Comment'].map(lambda x: re.sub(r'[?|!|\\'|\"|#]',r'', str(x)))\n",
        "train['Comment'] = train['Comment'].map(lambda x: re.sub(r'[.|,|)|(|\\|/]',r' ', str(x)))\n",
        "train['Comment'] = train['Comment'].apply(lambda x: remove_stopwords(x))\n",
        "\n",
        "test['Comment'] = test['Comment'].map(lambda x: re.sub('\\\\n',' ',str(x)))\n",
        "test['Comment'] = test['Comment'].map(lambda x: re.sub(r'\\W',' ',str(x)))\n",
        "test['Comment'] = test['Comment'].map(lambda x: re.sub(r'\\s+[a-zA-Z]\\s+',' ',str(x)))\n",
        "test['Comment'] = test['Comment'].map(lambda x: re.sub(r'\\^[a-zA-Z]\\s+',' ',str(x)))\n",
        "test['Comment'] = test['Comment'].map(lambda x: re.sub(r'\\s+',' ',str(x)))\n",
        "test['Comment'] = test['Comment'].str.lower()\n",
        "\n",
        "test['Comment'] = test['Comment'].map(lambda x: re.sub(r\"\\’\", \"\\'\", str(x)))\n",
        "\n",
        "test['Comment'] = test['Comment'].map(lambda x: re.sub(r'[?|!|\\'|\"|#]',r'', str(x)))\n",
        "test['Comment'] = test['Comment'].map(lambda x: re.sub(r'[.|,|)|(|\\|/]',r' ', str(x)))\n",
        "test['Comment'] = test['Comment'].apply(lambda x: remove_stopwords(x))"
      ],
      "metadata": {
        "id": "fsgrkVKcuQds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    '''make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
        "    #text = text.lower()\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    text = re.sub('[‘’“”…]', '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('user ', '', text)\n",
        "    text = re.sub('amp ', '', text)\n",
        "    text = re.sub('like ', '', text)\n",
        "    text = re.sub('new ', '', text)\n",
        "    text = re.sub('people ', '', text)\n",
        "    text = re.sub('bihday', 'birthday', text)\n",
        "    text = re.sub('allahsoil', 'allah soil', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "a1bbLYlRuQa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['Comment'] = train['Comment'].apply(lambda x: clean_text(x))\n",
        "test['Comment'] = test['Comment'].apply(lambda x: clean_text(x))"
      ],
      "metadata": {
        "id": "M4Fw0CGIuQYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train.copy()\n",
        "\n",
        "label = {0: 'A', 1: 'B'}\n",
        "train['label'] = train['label'].map(label)\n",
        "train = train.drop('Serial', axis = 1)\n",
        "\n",
        "train = pd.get_dummies(train, columns = ['label'])\n",
        "train.head()"
      ],
      "metadata": {
        "id": "rlp13E99uQVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categories = ['label_A', 'label_B']\n",
        "\n",
        "train_dict = {}\n",
        "\n",
        "for column in categories:\n",
        "    a = train.loc[train[column] == 1, 'Comment'].tolist()\n",
        "    train_dict[column] = ' '.join(a)"
      ],
      "metadata": {
        "id": "q9Kq2fPQuQR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can either keep it in dictionary format or put it into a pandas dataframe\n",
        "\n",
        "data_df = pd.DataFrame(train_dict.items())\n",
        "data_df.columns = ['index', 'Comment']\n",
        "data_df = data_df.set_index('index')\n",
        "data_df = data_df.sort_index()\n",
        "data_df.head()"
      ],
      "metadata": {
        "id": "zYY17rTkuQO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df = pd.DataFrame(data_df['Comment'].apply(lambda x: clean_text(x)))\n",
        "data_clean = data_df.copy()\n",
        "data_df.head()"
      ],
      "metadata": {
        "id": "HM9bTKzwuQLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train & Test splitting the Data"
      ],
      "metadata": {
        "id": "9FvPRpp2ZndS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x, y_hate, test_size = 0.30, random_state = 1)"
      ],
      "metadata": {
        "id": "v5lgPpLFYCX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing With Different Models"
      ],
      "metadata": {
        "id": "1B5S1MwxZxMi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Naive Bayes"
      ],
      "metadata": {
        "id": "X05pJTc0Z3Of"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Naive Bayes Classifier\n",
        "\n",
        "classifier_np = GaussianNB()\n",
        "\n",
        "# Start timer\n",
        "start_time = time.time()\n",
        "\n",
        "classifier_np.fit(x_train, y_train)\n",
        "\n",
        "# End timer\n",
        "end_time = time.time()"
      ],
      "metadata": {
        "id": "-oNazFiGYCVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate training time\n",
        "training_time = end_time - start_time\n",
        "\n",
        "print(f\"Training time of Naive Bayes: {training_time:.6f} seconds\")"
      ],
      "metadata": {
        "id": "TC9BLRUpYBGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model to disk\n",
        "filename = 'twitter_with_two_dataset_model_GaussianNB.sav'\n",
        "pickle.dump(classifier_np, open(filename, 'wb'))"
      ],
      "metadata": {
        "id": "Rpv8zuPpYBDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Naive Bayes\n",
        "y_pred_np = classifier_np.predict(x_test)\n",
        "cm = confusion_matrix(y_test, y_pred_np)\n",
        "print(cm)\n",
        "\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix(y_test, y_pred_np, labels = [1,0]), display_labels = [True, False])\n",
        "cm_display.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XCnwTKlNaX3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_test,y_pred_np))\n",
        "print(classification_report(y_test, y_pred_np))"
      ],
      "metadata": {
        "id": "O5SZa-Vbh3tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Decision Tree"
      ],
      "metadata": {
        "id": "NLSTP4-QaIl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Decision Tree\n",
        "\n",
        "classifier_dt = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
        "\n",
        "# Start timer\n",
        "start_time = time.time()\n",
        "\n",
        "classifier_dt.fit(x_train, y_train)\n",
        "\n",
        "# End timer\n",
        "end_time = time.time()"
      ],
      "metadata": {
        "id": "BHRXUbQcYA__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate training time\n",
        "training_time = end_time - start_time\n",
        "\n",
        "print(f\"Training time of Decision Tree: {training_time:.6f} seconds\")"
      ],
      "metadata": {
        "id": "rAvUxjzSYA8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model to disk\n",
        "filename = 'twitter_with_two_dataset_model_Decision_Tree.sav'\n",
        "pickle.dump(classifier_dt, open(filename, 'wb'))"
      ],
      "metadata": {
        "id": "mhg0tiXRYA5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Decision Tree\n",
        "y_pred_dt = classifier_dt.predict(x_test)\n",
        "cm = confusion_matrix(y_test, y_pred_dt)\n",
        "print(cm)\n",
        "\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix(y_test, y_pred_dt, labels = [1,0]), display_labels = [True, False])\n",
        "cm_display.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "p2fRJIJfYA2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "print(confusion_matrix(y_test,y_pred_np))\n",
        "print(classification_report(y_test, y_pred_np))"
      ],
      "metadata": {
        "id": "6V068BA5iJAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_test,y_pred_dt))\n",
        "print(classification_report(y_test, y_pred_dt))"
      ],
      "metadata": {
        "id": "yuJn3EwoiTLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#K-Nearest Neighbors"
      ],
      "metadata": {
        "id": "GAD6OFEja1a4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using KNN\n",
        "\n",
        "classifier_knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
        "\n",
        "# Start timer\n",
        "start_time = time.time()\n",
        "\n",
        "classifier_knn.fit(x_train, y_train)\n",
        "\n",
        "# End timer\n",
        "end_time = time.time()"
      ],
      "metadata": {
        "id": "wtGkcu2aYAz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate training time\n",
        "training_time = end_time - start_time\n",
        "\n",
        "print(f\"Training time of KNN: {training_time:.6f} seconds\")"
      ],
      "metadata": {
        "id": "iFXe8nC-YAw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model to disk\n",
        "filename = 'twitter_with_two_dataset_model_KNN.sav'\n",
        "pickle.dump(classifier_knn, open(filename, 'wb'))"
      ],
      "metadata": {
        "id": "s68B_eBva0wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#KNN\n",
        "y_pred_knn = classifier_knn.predict(x_test)\n",
        "cm = confusion_matrix(y_test, y_pred_knn)\n",
        "print(cm)\n",
        "\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix(y_test, y_pred_knn, labels = [1,0]), display_labels = [True, False])\n",
        "cm_display.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UXGAAUGCa0rH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_test,y_pred_knn))\n",
        "print(classification_report(y_test, y_pred_knn))"
      ],
      "metadata": {
        "id": "8jx44qokibR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Logistic Regression"
      ],
      "metadata": {
        "id": "oXIYS5ShbT-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Logistic Regression\n",
        "\n",
        "classifier_lr = LogisticRegression(random_state = 0)\n",
        "\n",
        "# Start timer\n",
        "start_time = time.time()\n",
        "\n",
        "classifier_lr.fit(x_train, y_train)\n",
        "\n",
        "# End timer\n",
        "end_time = time.time()"
      ],
      "metadata": {
        "id": "0JrX2z4Ia0nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate training time\n",
        "training_time = end_time - start_time\n",
        "\n",
        "print(f\"Training time of Logistic Regression: {training_time:.6f} seconds\")"
      ],
      "metadata": {
        "id": "FY-FpQVWa0kW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model to disk\n",
        "filename = 'twitter_with_two_dataset_model_Logistic_Regression.sav'\n",
        "pickle.dump(classifier_lr, open(filename, 'wb'))"
      ],
      "metadata": {
        "id": "Ulx6NSDAa0hN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Logistic Regression\n",
        "y_pred_lr=classifier_lr.predict(x_test)\n",
        "cm = confusion_matrix(y_test, y_pred_lr)\n",
        "print(cm)\n",
        "\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix(y_test, y_pred_lr, labels = [1,0]), display_labels = [True, False])\n",
        "cm_display.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PDUR07Ypa0dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_test,y_pred_lr))\n",
        "print(classification_report(y_test, y_pred_lr))"
      ],
      "metadata": {
        "id": "XZIeK-9Hif7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random Forest"
      ],
      "metadata": {
        "id": "mckcQgUtb1Cm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Random Forest\n",
        "\n",
        "classifier_rf = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
        "\n",
        "# Start timer\n",
        "start_time = time.time()\n",
        "\n",
        "classifier_rf.fit(x_train, y_train)\n",
        "\n",
        "# End timer\n",
        "end_time = time.time()"
      ],
      "metadata": {
        "id": "NoTvp-wQa0az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate training time\n",
        "training_time = end_time - start_time\n",
        "\n",
        "print(f\"Training time of Random Forest: {training_time:.6f} seconds\")"
      ],
      "metadata": {
        "id": "AYaIOCaCa0Xe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model to disk\n",
        "filename = 'twitter_with_two_dataset_model_Random_Forest.sav'\n",
        "pickle.dump(classifier_rf, open(filename, 'wb'))"
      ],
      "metadata": {
        "id": "1UU110RDa0UU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Random Forest\n",
        "y_pred_rf = classifier_rf.predict(x_test)\n",
        "cm = confusion_matrix(y_test, y_pred_rf)\n",
        "print(cm)\n",
        "\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix(y_test, y_pred_rf, labels = [1,0]), display_labels = [True, False])\n",
        "cm_display.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JqPO5Caccfiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_test,y_pred_np))\n",
        "print(classification_report(y_test, y_pred_rf))"
      ],
      "metadata": {
        "id": "ehk-fbtSiwCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SVM"
      ],
      "metadata": {
        "id": "_VveKQxhcByB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import time\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "6I6Q98jfqseX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming your data is already preprocessed and you have x_train, x_test, y_train, y_test\n",
        "\n",
        "# Using SVM\n",
        "classifier_svm = SVC(kernel = 'linear', random_state = 0)"
      ],
      "metadata": {
        "id": "jeQuYn8yquJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start timer\n",
        "start_time = time.time()\n",
        "\n",
        "classifier_svm.fit(x_train, y_train)\n"
      ],
      "metadata": {
        "id": "GbCq_joSq1SX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# End timer\n",
        "end_time = time.time()\n",
        "training_time = end_time - start_time\n",
        "\n",
        "print(f\"Training time of SVM: {training_time:.6f} seconds\")\n"
      ],
      "metadata": {
        "id": "hhOtHQrRq-se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model to disk\n",
        "filename = 'twitter_with_two_dataset_model_SVM.sav'\n",
        "pickle.dump(classifier_svm, open(filename, 'wb'))\n"
      ],
      "metadata": {
        "id": "CU5N4Yv3rA0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM predictions\n",
        "y_pred_svm = classifier_svm.predict(x_test)"
      ],
      "metadata": {
        "id": "si4oy92srE5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix and evaluation\n",
        "cm = confusion_matrix(y_test, y_pred_svm)\n",
        "print(cm)\n"
      ],
      "metadata": {
        "id": "aksWdge4rEom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display confusion matrix\n",
        "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred_svm, labels=[1, 0]), display_labels=[True, False])\n",
        "cm_display.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y8pDuhU_rOOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print confusion matrix and classification report\n",
        "print(confusion_matrix(y_test, y_pred_svm))\n",
        "print(classification_report(y_test, y_pred_svm))"
      ],
      "metadata": {
        "id": "t-HyeIv2rSV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generating the Accuracy & F1 Score"
      ],
      "metadata": {
        "id": "NuVhf_5Tc8bN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rf_score = accuracy_score(y_test, y_pred_rf)\n",
        "knn_score = accuracy_score(y_test, y_pred_knn)\n",
        "#svm_score = accuracy_score(y_test, y_pred_svm)\n",
        "lr_score = accuracy_score(y_test, y_pred_lr)\n",
        "dt_score = accuracy_score(y_test, y_pred_dt)\n",
        "np_score = accuracy_score(y_test, y_pred_np)\n",
        "\n",
        "print ('--' * 20)\n",
        "print('Random Forest Accuracy: ', str(rf_score))\n",
        "print('F1 score: ', f1_score(y_test, y_pred_rf, labels = [1,0]))\n",
        "print ('--' * 20)\n",
        "print ('')\n",
        "print ('--' * 20)\n",
        "print('K-Nearest Neighbors Accuracy: ', str(knn_score))\n",
        "print('F1 score: ', f1_score(y_test, y_pred_knn, labels = [1,0]))\n",
        "print ('--' * 20)\n",
        "print ('')\n",
        "'''print ('--' * 20)\n",
        "print('Support Vector Machine Accuracy: ', str(svm_score))\n",
        "print('F1 score: ', f1_score(y_test, y_pred_svm, labels = [1,0]))\n",
        "print ('--' * 20)\n",
        "print ('')'''\n",
        "print ('--' * 20)\n",
        "print('Logistic Regression Accuracy: ',str(lr_score))\n",
        "print('F1 score: ', f1_score(y_test, y_pred_lr, labels = [1,0]))\n",
        "print ('--' * 20)\n",
        "print ('')\n",
        "print ('--' * 20)\n",
        "print('Decision Tree Accuracy: ', str(dt_score))\n",
        "print('F1 score: ', f1_score(y_test, y_pred_dt, labels = [1,0]))\n",
        "print ('--' * 20)\n",
        "print ('')\n",
        "print ('--' * 20)\n",
        "print('Naive Bayes Accuracy: ', str(np_score))\n",
        "print('F1 score: ', f1_score(y_test, y_pred_np, labels = [1,0]))\n",
        "print ('--' * 20)\n",
        "print ('')"
      ],
      "metadata": {
        "id": "6jwz3Uwsaz_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating and Training an LSTM Model"
      ],
      "metadata": {
        "id": "YdDLMQJNjulC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "EMiI-YDOcUEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Text preprocessing\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df['Comment'])\n",
        "sequences = tokenizer.texts_to_sequences(df['Comment'])\n",
        "max_sequence_length = 100  # Define an appropriate max sequence length\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')\n"
      ],
      "metadata": {
        "id": "Zw5VVDzPcUBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model architecture\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 100  # You can adjust this dimension\n",
        "lstm_units = 128  # You can adjust the number of units\n",
        "num_classes = 3  # Adjust based on your number of classes"
      ],
      "metadata": {
        "id": "57o2PJ1zcT-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "model.add(LSTM(units=lstm_units))\n",
        "model.add(Dense(units=num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "wiXr9O4OcT4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset\n",
        "X_train, X_val, y_train, y_val = train_test_split(padded_sequences, df['label'], test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "bqjJNu8acTyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "num_epochs = 10  # Adjust as needed\n",
        "batch_size = 32  # Adjust as needed\n",
        "\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=num_epochs, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "YV8siIBwoFCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_val, y_val)\n",
        "print(\"Validation Loss:\", loss)\n",
        "print(\"Validation Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "GmEfCO48oE_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O55VjfGSoE59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XuBybw0RoE0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LCJuO5QacTud"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}